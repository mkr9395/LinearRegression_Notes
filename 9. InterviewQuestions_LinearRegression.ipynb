{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ecc376",
   "metadata": {},
   "source": [
    "**Easy Level:**\n",
    "\n",
    "1. What is linear regression?\n",
    "2. Explain the difference between simple linear regression and multiple linear regression.\n",
    "3. What is the goal of linear regression?\n",
    "4. How do you represent a linear regression equation mathematically?\n",
    "5. What is the role of the dependent variable in linear regression?\n",
    "6. Define the terms \"independent variable\" and \"coefficient\" in the context of linear regression.\n",
    "7. What is the least squares method, and how is it used in linear regression?\n",
    "8. What is the difference between the intercept and slope in a linear regression equation?\n",
    "9. Explain the concept of residuals in linear regression.\n",
    "10. How do you assess the goodness of fit in linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f32237",
   "metadata": {},
   "source": [
    "\n",
    "**1. What is linear regression?**\n",
    "\n",
    "\n",
    "**Linear regression** is a statistical method used to model the relationship between one dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Explain the difference between simple linear regression and multiple linear regression.**\n",
    "\n",
    "* **Simple Linear Regression**: Involves **one independent variable** and one dependent variable.\n",
    "\n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "  $$\n",
    "* **Multiple Linear Regression**: Involves **two or more independent variables** predicting a single dependent variable.\n",
    "\n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**3. What is the goal of linear regression?**\n",
    "The goal is to **find the best-fitting linear relationship** between the input features and the output variable, so that we can:\n",
    "\n",
    "* Understand the strength and direction of relationships.\n",
    "* Predict outcomes for new data.\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you represent a linear regression equation mathematically?**\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $y$: Dependent variable\n",
    "* $\\beta_0$: Intercept\n",
    "* $\\beta_1, \\dots, \\beta_n$: Coefficients\n",
    "* $x_1, \\dots, x_n$: Independent variables\n",
    "* $\\epsilon$: Error term\n",
    "\n",
    "---\n",
    "\n",
    "**5. What is the role of the dependent variable in linear regression?**\n",
    "The **dependent variable** (also called target or output) is the variable the model is trying to predict or explain based on the independent variables.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Define the terms \"independent variable\" and \"coefficient\" in the context of linear regression.**\n",
    "\n",
    "* **Independent variable**: Input variable(s) that influence the dependent variable.\n",
    "* **Coefficient ($\\beta_i$)**: Represents the weight or influence of each independent variable on the dependent variable. A change of 1 unit in $x_i$ leads to a $\\beta_i$ change in $y$, all else constant.\n",
    "\n",
    "---\n",
    "\n",
    "**7. What is the least squares method, and how is it used in linear regression?**\n",
    "The **least squares method** minimizes the **sum of squared residuals** (differences between actual and predicted values):\n",
    "\n",
    "$$\n",
    "\\min \\sum (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "This method helps determine the optimal coefficients for the linear regression model.\n",
    "\n",
    "---\n",
    "\n",
    "**8. What is the difference between the intercept and slope in a linear regression equation?**\n",
    "\n",
    "* **Intercept ($\\beta_0$)**: Value of $y$ when all independent variables are zero. It shifts the regression line up or down.\n",
    "* **Slope ($\\beta_i$)**: Represents the change in $y$ for a one-unit change in $x_i$. It determines the direction and steepness of the line.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Explain the concept of residuals in linear regression.**\n",
    "**Residuals** are the differences between the actual and predicted values:\n",
    "\n",
    "$$\n",
    "\\text{Residual} = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "They represent the error or noise the model couldn’t explain.\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you assess the goodness of fit in linear regression?**\n",
    "Common metrics:\n",
    "\n",
    "* **R-squared ($R^2$)**: Proportion of variance in $y$ explained by $X$; ranges from 0 to 1.\n",
    "* **Adjusted R-squared**: Adjusts $R^2$ for number of predictors.\n",
    "* **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)**: Measure average prediction error.\n",
    "* **Residual plots**: Visual inspection of model performance and assumption validity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5519e",
   "metadata": {},
   "source": [
    "**Medium Level:**\n",
    "\n",
    "11. What are the assumptions of linear regression?\n",
    "12. How do you handle multicollinearity in multiple linear regression?\n",
    "13. Describe the process of model selection in linear regression.\n",
    "14. What is the purpose of the coefficient of determination (R-squared) in linear regression?\n",
    "15. How do you interpret the p-value in linear regression?\n",
    "16. Explain the concept of homoscedasticity and its implications in linear regression.\n",
    "17. What is regularization, and why might it be applied to linear regression?\n",
    "18. Discuss the difference between L1 and L2 regularization techniques in linear regression.\n",
    "19. What is the normality assumption in linear regression, and how is it tested?\n",
    "20. How can you deal with outliers in linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba34d67",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **11. What are the assumptions of linear regression?**\n",
    "\n",
    "1. **Linearity** – The relationship between independent and dependent variables is linear.\n",
    "2. **Independence** – Observations are independent of each other.\n",
    "3. **Homoscedasticity** – Constant variance of errors across all levels of independent variables.\n",
    "4. **Normality of errors** – The residuals (errors) should be normally distributed.\n",
    "5. **No multicollinearity** – Independent variables should not be highly correlated with each other.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. How do you handle multicollinearity in multiple linear regression?**\n",
    "\n",
    "* **Remove correlated predictors**.\n",
    "* **Combine variables** using techniques like PCA.\n",
    "* **Use regularization** (e.g., Ridge regression).\n",
    "* **Check Variance Inflation Factor (VIF)** to detect multicollinearity (VIF > 5 or 10 indicates issues).\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Describe the process of model selection in linear regression.**\n",
    "\n",
    "1. **Feature selection** – Choose relevant features (manual or automated).\n",
    "2. **Fit multiple models** – Try different combinations of predictors.\n",
    "3. **Evaluate metrics** – Use R², Adjusted R², AIC, BIC, RMSE.\n",
    "4. **Cross-validation** – Check how the model generalizes to unseen data.\n",
    "5. **Choose best tradeoff** between simplicity and performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **14. What is the purpose of the coefficient of determination (R-squared) in linear regression?**\n",
    "\n",
    "* Measures the **proportion of variance** in the dependent variable explained by the model.\n",
    "* Ranges from **0 to 1**.\n",
    "* Higher R² means better fit, but **does not guarantee causation or no overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### **15. How do you interpret the p-value in linear regression?**\n",
    "\n",
    "* It tests whether a coefficient is significantly different from zero.\n",
    "* **Low p-value (< 0.05)** → The predictor is statistically significant.\n",
    "* **High p-value** → Not enough evidence that the predictor impacts the dependent variable.\n",
    "\n",
    "---\n",
    "\n",
    "### **16. Explain the concept of homoscedasticity and its implications in linear regression.**\n",
    "\n",
    "* Homoscedasticity = **Equal variance of residuals** across all values of predictors.\n",
    "* If violated (i.e., heteroscedasticity), it can:\n",
    "\n",
    "  * Distort standard errors\n",
    "  * Lead to **invalid confidence intervals** and **p-values**\n",
    "* Detected using **residual plots** or **Breusch-Pagan test**.\n",
    "\n",
    "---\n",
    "\n",
    "### **17. What is regularization, and why might it be applied to linear regression?**\n",
    "\n",
    "* Regularization adds a **penalty** to the loss function to discourage large coefficients.\n",
    "* Helps prevent **overfitting**, especially when:\n",
    "\n",
    "  * There are many features\n",
    "  * Features are correlated\n",
    "* Common types: **Ridge (L2)** and **Lasso (L1)** regression.\n",
    "\n",
    "---\n",
    "\n",
    "### **18. Discuss the difference between L1 and L2 regularization techniques in linear regression.**\n",
    "\n",
    "| Feature           | L1 (Lasso)             | L2 (Ridge)        |\n",
    "| ----------------- | ---------------------- | ----------------- |\n",
    "| Penalty           | Sum of absolute values | Sum of squares    |\n",
    "| Feature Selection | Yes (can shrink to 0)  | No                |\n",
    "| Sparse model?     | Yes                    | No                |\n",
    "| Use case          | Feature reduction      | Multicollinearity |\n",
    "\n",
    "---\n",
    "\n",
    "### **19. What is the normality assumption in linear regression, and how is it tested?**\n",
    "\n",
    "* Assumes that the **residuals (errors)** follow a **normal distribution**.\n",
    "* Important for valid **confidence intervals** and **hypothesis tests**.\n",
    "* Tested using:\n",
    "\n",
    "  * **Histogram** or **Q-Q plot** of residuals\n",
    "  * **Shapiro-Wilk** or **Kolmogorov–Smirnov test**\n",
    "\n",
    "---\n",
    "\n",
    "### **20. How can you deal with outliers in linear regression?**\n",
    "\n",
    "* **Identify** using boxplots, residual plots, or leverage scores (e.g., Cook's distance).\n",
    "* **Solutions**:\n",
    "\n",
    "  * Remove them (if truly anomalous)\n",
    "  * Use **robust regression** (e.g., Huber regression)\n",
    "  * **Transform variables** (e.g., log transformation)\n",
    "  * Cap or winsorize the values\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f4c6c",
   "metadata": {},
   "source": [
    "**Hard Level:**\n",
    "\n",
    "21. Describe the gradient descent algorithm and its use in linear regression.\n",
    "22. What are the potential issues with using linear regression for a dataset with a nonlinear relationship?\n",
    "23. Explain the bias-variance trade-off in the context of linear regression.\n",
    "24. Discuss the concept of ridge regression and its advantages over ordinary least squares (OLS) regression.\n",
    "25. What is the purpose of feature scaling or normalization in linear regression?\n",
    "26. How does cross-validation help in evaluating the performance of a linear regression model?\n",
    "27. Describe the differences between linear regression and logistic regression.\n",
    "28. What is the impact of outliers on the coefficients and predictions of a linear regression model?\n",
    "29. Explain the concept of heteroscedasticity and how to address it in linear regression.\n",
    "30. What is the difference between ridge regression and LASSO regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182af4f",
   "metadata": {},
   "source": [
    "\n",
    "### **21. Describe the gradient descent algorithm and its use in linear regression.**\n",
    "\n",
    "* **Gradient Descent** is an optimization algorithm that minimizes the cost function (e.g., Mean Squared Error).\n",
    "* It updates parameters (weights) iteratively:\n",
    "\n",
    "$$\n",
    "\\text{new } w = w - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "* In **linear regression**, it finds the best-fitting line by reducing the prediction error.\n",
    "\n",
    "> Used when analytical solutions (like the Normal Equation) are too expensive or infeasible for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **22. What are the potential issues with using linear regression for a dataset with a nonlinear relationship?**\n",
    "\n",
    "* Linear regression assumes a straight-line relationship.\n",
    "* On nonlinear data:\n",
    "\n",
    "  * **Poor fit** and high residuals\n",
    "  * **Low R² score**\n",
    "  * **Biased predictions**\n",
    "* Solution: Use **polynomial regression** or **nonlinear models** (like decision trees, SVR, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### **23. Explain the bias-variance trade-off in the context of linear regression.**\n",
    "\n",
    "* **Bias**: Error from incorrect assumptions (e.g., assuming linearity).\n",
    "* **Variance**: Error from sensitivity to small fluctuations in training data.\n",
    "* **Trade-off**:\n",
    "\n",
    "  * **High bias** → underfitting\n",
    "  * **High variance** → overfitting\n",
    "\n",
    "Linear regression has **low variance, high bias**, making it stable but sometimes underfits.\n",
    "\n",
    "---\n",
    "\n",
    "### **24. Discuss the concept of ridge regression and its advantages over ordinary least squares (OLS) regression.**\n",
    "\n",
    "* **Ridge Regression** adds a penalty to large coefficients:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{MSE} + \\lambda \\sum w^2\n",
    "$$\n",
    "\n",
    "* **Advantages**:\n",
    "\n",
    "  * Reduces **overfitting**\n",
    "  * Handles **multicollinearity**\n",
    "  * Keeps all features (shrinks but doesn’t eliminate)\n",
    "\n",
    "---\n",
    "\n",
    "### **25. What is the purpose of feature scaling or normalization in linear regression?**\n",
    "\n",
    "* Ensures all features contribute **equally** to the model.\n",
    "* Especially important for:\n",
    "\n",
    "  * **Gradient descent** (converges faster)\n",
    "  * **Regularized models** (like Ridge/Lasso) so no feature dominates the penalty term.\n",
    "* Methods: **Standardization** (z-score), **Min-Max scaling**\n",
    "\n",
    "---\n",
    "\n",
    "### **26. How does cross-validation help in evaluating the performance of a linear regression model?**\n",
    "\n",
    "* Cross-validation (e.g., **k-fold CV**) splits data into **training + validation sets** multiple times.\n",
    "* Provides a more **robust estimate** of model performance.\n",
    "* Helps detect:\n",
    "\n",
    "  * **Overfitting** (train ≫ validation error)\n",
    "  * **Underfitting** (both errors high)\n",
    "* Improves **generalizability**.\n",
    "\n",
    "---\n",
    "\n",
    "### **27. Describe the differences between linear regression and logistic regression.**\n",
    "\n",
    "| Aspect        | Linear Regression             | Logistic Regression                |\n",
    "| ------------- | ----------------------------- | ---------------------------------- |\n",
    "| Output        | Continuous                    | Binary (0/1)                       |\n",
    "| Function      | Linear                        | Sigmoid (S-shaped)                 |\n",
    "| Loss Function | Mean Squared Error            | Log Loss (Cross-Entropy)           |\n",
    "| Use Case      | Predict values (e.g., prices) | Classification (e.g., spam vs not) |\n",
    "\n",
    "---\n",
    "\n",
    "### **28. What is the impact of outliers on the coefficients and predictions of a linear regression model?**\n",
    "\n",
    "* **Outliers** have a **large influence** on the fitted line.\n",
    "* Can **distort coefficients**, reducing model accuracy.\n",
    "* Predictions become unreliable.\n",
    "* Solutions:\n",
    "\n",
    "  * **Remove or cap outliers**\n",
    "  * Use **robust regression**\n",
    "  * **Log-transform** or use **median-based techniques**\n",
    "\n",
    "---\n",
    "\n",
    "### **29. Explain the concept of heteroscedasticity and how to address it in linear regression.**\n",
    "\n",
    "* **Heteroscedasticity**: Non-constant variance of residuals.\n",
    "\n",
    "  * Violates regression assumptions\n",
    "  * Leads to **inefficient estimates** and **biased standard errors**\n",
    "* Detection:\n",
    "\n",
    "  * Residual plots\n",
    "  * Breusch–Pagan test\n",
    "* Solutions:\n",
    "\n",
    "  * **Log-transform** the dependent variable\n",
    "  * Use **Weighted Least Squares**\n",
    "  * Switch to **robust regression**\n",
    "\n",
    "---\n",
    "\n",
    "### **30. What is the difference between ridge regression and LASSO regression?**\n",
    "\n",
    "| Feature          | Ridge (L2)                   | Lasso (L1)                         |   |   |\n",
    "| ---------------- | ---------------------------- | ---------------------------------- | - | - |\n",
    "| Penalty Term     | $\\lambda \\sum w^2$           | (\\lambda \\sum                      | w | ) |\n",
    "| Shrinks to Zero? | No                           | Yes (feature selection)            |   |   |\n",
    "| Use Case         | Multicollinearity            | Sparse models, feature elimination |   |   |\n",
    "| Solution Type    | Always has a unique solution | May zero out some coefficients     |   |   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f4fc5",
   "metadata": {},
   "source": [
    "**Advanced Level:**\n",
    "\n",
    "31. How can you handle missing data in linear regression?\n",
    "32. Discuss the concept of endogeneity in linear regression and how to address it.\n",
    "33. What is the Durbin-Watson statistic, and what does it measure in linear regression?\n",
    "34. Explain the concept of autocorrelation in the residuals of a time series linear regression model.\n",
    "35. What are generalized linear models (GLMs), and how do they extend linear regression?\n",
    "36. Describe the assumptions and applications of logistic regression in contrast to linear regression.\n",
    "37. How can you perform feature selection in linear regression effectively?\n",
    "38. Discuss the differences between forward, backward, and stepwise regression selection methods.\n",
    "39. What is the concept of regularized linear regression, and how does it relate to ridge and LASSO regression?\n",
    "40. Can you implement linear regression from scratch in Python or another programming language?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42752133",
   "metadata": {},
   "source": [
    "\n",
    "**31. How can you handle missing data in linear regression?**\n",
    "Handling missing data is critical for ensuring model accuracy. Common strategies include:\n",
    "\n",
    "* **Deletion**:\n",
    "\n",
    "  * *Listwise deletion*: Remove rows with any missing values (if missingness is MCAR).\n",
    "* **Imputation**:\n",
    "\n",
    "  * *Mean/Median imputation*: For numerical data.\n",
    "  * *KNN or regression-based imputation*: More accurate but complex.\n",
    "  * *Multiple Imputation*: Generates multiple imputations and combines estimates.\n",
    "* **Modeling missingness**: Use missingness as a feature if it may carry information.\n",
    "\n",
    "---\n",
    "\n",
    "**32. Discuss the concept of endogeneity in linear regression and how to address it.**\n",
    "**Endogeneity** occurs when an independent variable is correlated with the error term. It violates the assumption of exogeneity, leading to biased and inconsistent estimates.\n",
    "**Causes**:\n",
    "\n",
    "* Omitted variable bias\n",
    "* Simultaneity (mutual causality)\n",
    "* Measurement error\n",
    "  **Solutions**:\n",
    "* **Instrumental Variables (IV)**: Use instruments uncorrelated with the error term but correlated with the endogenous regressor.\n",
    "* **Two-stage least squares (2SLS)**: A common IV-based method.\n",
    "* **Control function approach**: Adjusts the regression to account for endogeneity.\n",
    "\n",
    "---\n",
    "\n",
    "**33. What is the Durbin-Watson statistic, and what does it measure in linear regression?**\n",
    "The **Durbin-Watson (DW) statistic** tests for **autocorrelation in the residuals**, particularly first-order serial correlation.\n",
    "**Formula**:\n",
    "\n",
    "$$\n",
    "DW = \\frac{\\sum_{t=2}^n (e_t - e_{t-1})^2}{\\sum_{t=1}^n e_t^2}\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* DW ≈ 2 → No autocorrelation\n",
    "* DW < 2 → Positive autocorrelation\n",
    "* DW > 2 → Negative autocorrelation\n",
    "\n",
    "---\n",
    "\n",
    "**34. Explain the concept of autocorrelation in the residuals of a time series linear regression model.**\n",
    "**Autocorrelation (serial correlation)** occurs when residuals are correlated across time.\n",
    "**Why it matters**:\n",
    "\n",
    "* Violates the assumption of independent errors.\n",
    "* Leads to underestimated standard errors → inflated t-stats → misleading significance.\n",
    "  **Detection**:\n",
    "* Durbin-Watson test\n",
    "* ACF/PACF plots\n",
    "  **Remedies**:\n",
    "* Include lagged variables\n",
    "* Use time series models (e.g., ARIMA)\n",
    "* Apply GLS (Generalized Least Squares)\n",
    "\n",
    "---\n",
    "\n",
    "**35. What are generalized linear models (GLMs), and how do they extend linear regression?**\n",
    "GLMs **extend linear regression** by allowing:\n",
    "\n",
    "* **Response variables** with error distributions other than normal (e.g., binomial, Poisson).\n",
    "* A **link function** to model the relationship between predictors and the expected value of the response.\n",
    "  **Components**:\n",
    "\n",
    "1. **Random component**: Distribution of response (e.g., binomial, Poisson)\n",
    "2. **Systematic component**: Linear combination of inputs\n",
    "3. **Link function**: Connects the mean of the response to the linear predictor\n",
    "   **Examples**:\n",
    "\n",
    "* Logistic regression (logit link + binomial)\n",
    "* Poisson regression (log link + Poisson)\n",
    "\n",
    "---\n",
    "\n",
    "**36. Describe the assumptions and applications of logistic regression in contrast to linear regression.**\n",
    "**Logistic Regression** is used for **binary classification**, not regression.\n",
    "**Key differences**:\n",
    "\n",
    "* **Output**: Probability (0–1), not continuous value\n",
    "* **Loss function**: Log-likelihood, not MSE\n",
    "* **Link function**: Logistic (sigmoid)\n",
    "\n",
    "**Assumptions of Logistic Regression**:\n",
    "\n",
    "* Independent observations\n",
    "* Linear relationship between independent variables and the **log-odds**\n",
    "* No multicollinearity\n",
    "* Large sample size\n",
    "  **Applications**:\n",
    "* Credit risk prediction\n",
    "* Disease classification\n",
    "* Email spam detection\n",
    "\n",
    "---\n",
    "\n",
    "**37. How can you perform feature selection in linear regression effectively?**\n",
    "Methods include:\n",
    "\n",
    "* **Filter Methods**:\n",
    "\n",
    "  * Correlation matrix\n",
    "  * Variance Threshold\n",
    "* **Wrapper Methods**:\n",
    "\n",
    "  * Forward selection\n",
    "  * Backward elimination\n",
    "  * Stepwise selection\n",
    "* **Embedded Methods**:\n",
    "\n",
    "  * Lasso (L1) regularization (shrinks some coefficients to zero)\n",
    "  * Recursive Feature Elimination (RFE)\n",
    "* **Model-based**:\n",
    "\n",
    "  * Use feature importance from models (e.g., Random Forest)\n",
    "\n",
    "---\n",
    "\n",
    "**38. Discuss the differences between forward, backward, and stepwise regression selection methods.**\n",
    "\n",
    "* **Forward Selection**:\n",
    "  Starts with no variables, adds one at a time based on improvement in model (e.g., AIC, adjusted R²).\n",
    "\n",
    "* **Backward Elimination**:\n",
    "  Starts with all variables, removes the least significant one iteratively.\n",
    "\n",
    "* **Stepwise Selection**:\n",
    "  Combines forward and backward — adds/removes variables at each step.\n",
    "\n",
    "**Use**:\n",
    "\n",
    "* Forward: When few predictors are likely relevant.\n",
    "* Backward: When you start with many.\n",
    "* Stepwise: Balanced approach but prone to overfitting if not validated.\n",
    "\n",
    "---\n",
    "\n",
    "**39. What is the concept of regularized linear regression, and how does it relate to ridge and LASSO regression?**\n",
    "Regularization **adds a penalty** to the loss function to prevent overfitting and reduce variance.\n",
    "\n",
    "* **Ridge Regression (L2 penalty)**:\n",
    "\n",
    "  $$\n",
    "  \\text{Loss} = \\text{MSE} + \\lambda \\sum w_i^2\n",
    "  $$\n",
    "\n",
    "  Shrinks coefficients but doesn't make them exactly zero.\n",
    "\n",
    "* **LASSO Regression (L1 penalty)**:\n",
    "\n",
    "  $$\n",
    "  \\text{Loss} = \\text{MSE} + \\lambda \\sum |w_i|\n",
    "  $$\n",
    "\n",
    "  Performs both shrinkage and feature selection (some coefficients become exactly zero).\n",
    "\n",
    "* **Elastic Net**: Combines L1 and L2.\n",
    "\n",
    "---\n",
    "\n",
    "**40. Can you implement linear regression from scratch in Python or another programming language?**\n",
    "Yes, here's a basic Python example using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])  # add intercept term manually\n",
    "y = np.array([1, 2, 3])\n",
    "\n",
    "# Closed-form solution (Normal Equation): w = (X^T X)^-1 X^T y\n",
    "X_T_X = X.T @ X\n",
    "X_T_y = X.T @ y\n",
    "weights = np.linalg.inv(X_T_X) @ X_T_y\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "```\n",
    "\n",
    "This gives the coefficients of a simple linear regression model without using libraries like `scikit-learn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d947b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
